# -*- coding: utf-8 -*-
"""Multi-layer Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfQZa6loMg8gJITLp5g_cUal2FAK_170

# Multi-layer perceptron

## Sigmoid function

![alt text](https://drive.google.com/uc?id=1TOnW5t6niK8kUCjjv9Nh-cdxmYi_it-F)
"""

import numpy as np

def sigmoid(sum):
  return 1 / (1 + np.exp(-sum))

np.exp(1)

np.exp(2)

2.718281828459045 * 2.718281828459045

np.exp(0)

np.exp(-1)

1 / 2.718281828459045

sigmoid(50)

sigmoid(0)

sigmoid(30.5)

sigmoid(-25.5)

sigmoid(0.358), sigmoid(-0.577), sigmoid(-0.469)

sigmoid(-0.424), sigmoid(-0.740), sigmoid(-0.961)

sigmoid(-0.066), sigmoid(-1.317), sigmoid(-1.430)

sigmoid(-0.381)

sigmoid(-0.274)

sigmoid(-0.254)

sigmoid(-0.168)

"""## Input layer to hidden layer

![alt text](https://drive.google.com/uc?id=1GBajLXOuC8SgxKxf57iTrHm8Lw3MMqzz)
"""

inputs = np.array([[0,0],
                   [0,1],
                   [1,0],
                   [1,1]])
inputs.shape

outputs = np.array([[0],
                    [1],
                    [1],
                    [0]])
outputs.shape

weights0 = np.array([[-0.424, -0.740, -0.961],
                     [0.358, -0.577, -0.469]])
weights0.shape

weights1 = np.array([[-0.017],
                     [-0.893],
                     [0.148]])
weights1.shape

epochs = 100
learning_rate = 0.3

#for epoch in epochs:

input_layer = inputs
input_layer

sum_synapse0 = np.dot(input_layer, weights0)
sum_synapse0

hidden_layer = sigmoid(sum_synapse0)
hidden_layer

weights1

sum_synapse1 = np.dot(hidden_layer, weights1)
sum_synapse1

output_layer = sigmoid(sum_synapse1)
output_layer

"""![alt text](https://drive.google.com/uc?id=1tyaDx4fNdXIS9EJTcKO9hP2P03GCA0Fj)"""

outputs

output_layer

error_output_layer = outputs - output_layer
error_output_layer

average = np.mean(abs(error_output_layer))
average

"""![alt text](https://drive.google.com/uc?id=1TS68lw8fRrZptGsOZ4iBw4fw0mh1_yH7)"""

def sigmoid_derivative(sigmoid):
  return sigmoid * (1 - sigmoid)

s = sigmoid(0.5)
s

d = sigmoid_derivative(s)
d

sigmoid_derivative(0.405)

sigmoid_derivative(0.589)

"""![alt text](https://drive.google.com/uc?id=1onElTzv80-S7b2GqzP9XXENNTxYZKtbR)"""

output_layer

derivative_output = sigmoid_derivative(output_layer)
derivative_output

error_output_layer

delta_output = error_output_layer * derivative_output
delta_output

"""![alt text](https://drive.google.com/uc?id=1KwX3K5FUunqnneS3Z1zAreMmENdVfnig)"""

delta_output

weights1

delta_output_x_weight = delta_output.dot(weights1)

weights1T = weights1.T
weights1T

weights1.shape, weights1T.shape

delta_output

delta_output_x_weight = delta_output.dot(weights1T)
delta_output_x_weight

hidden_layer

delta_hidden_layer = delta_output_x_weight * sigmoid_derivative(hidden_layer)
delta_hidden_layer

"""![alt text](https://drive.google.com/uc?id=15LCxQTbvGr9eNwTjgHmMbBr1oBun51yr)"""

hidden_layer

delta_output

hidden_layerT = hidden_layer.T
hidden_layerT

input_x_delta1 = hidden_layerT.dot(delta_output)
input_x_delta1

weights1 = weights1 + (input_x_delta1 * learning_rate)
weights1

"""![alt text](https://drive.google.com/uc?id=116y4hCuK1_ek8RcyW5t8t6tZLGsk_D6x)"""

input_layer

delta_hidden_layer

input_layerT = input_layer.T
input_layerT

input_x_delta0 = input_layerT.dot(delta_hidden_layer)
input_x_delta0

weights0 = weights0 + (input_x_delta0 * learning_rate)
weights0

"""## Complete neural network"""

import numpy as np

def sigmoid(sum):
  return 1 / (1 + np.exp(-sum))

def sigmoid_derivative(sigmoid):
  return sigmoid * (1 - sigmoid)

inputs = np.array([[0,0], 
                   [0,1], 
                   [1,0], 
                   [1,1]])

outputs = np.array([[0],
                    [1],
                    [1],
                    [0]])

#weights0 = np.array([[-0.424, -0.740, -0.961],
#                     [0.358, -0.577, -0.469]])

#weights1 = np.array([[-0.017],
#                     [-0.893],
#                     [0.148]])

weights0 = 2 * np.random.random((2, 3)) - 1
weights1 = 2 * np.random.random((3, 1)) - 1

epochs = 100000
learning_rate = 0.6
error = []

for epoch in range(epochs):
  input_layer = inputs
  sum_synapse0 = np.dot(input_layer, weights0)
  hidden_layer = sigmoid(sum_synapse0)

  sum_synapse1 = np.dot(hidden_layer, weights1)
  output_layer = sigmoid(sum_synapse1)

  error_output_layer = outputs - output_layer
  average = np.mean(abs(error_output_layer))
  if epoch % 10000 == 0:
    print('Epoch: ' + str(epoch + 1) + ' Error: ' + str(average))
    error.append(average)
  
  derivative_output = sigmoid_derivative(output_layer)
  delta_output = error_output_layer * derivative_output
  
  weights1T = weights1.T
  delta_output_weight = delta_output.dot(weights1T)
  delta_hidden_layer = delta_output_weight * sigmoid_derivative(hidden_layer)
  
  hidden_layerT = hidden_layer.T
  input_x_delta1 = hidden_layerT.dot(delta_output)
  weights1 = weights1 + (input_x_delta1 * learning_rate)
  
  input_layerT = input_layer.T
  input_x_delta0 = input_layerT.dot(delta_hidden_layer)
  weights0 = weights0 + (input_x_delta0 * learning_rate)

1 - 0.020416844706173585

import matplotlib.pyplot as plt
plt.xlabel('Epochs')
plt.ylabel('Error')
plt.plot(error)

outputs

output_layer

weights0

weights1

def calculate_output(instance):
  hidden_layer = sigmoid(np.dot(instance, weights0))
  output_layer = sigmoid(np.dot(hidden_layer, weights1))
  return output_layer[0]

round(calculate_output(np.array([0, 0])))

round(calculate_output(np.array([0, 1])))

round(calculate_output(np.array([1, 0])))

round(calculate_output(np.array([1, 1])))